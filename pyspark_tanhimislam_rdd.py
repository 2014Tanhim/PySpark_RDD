# -*- coding: utf-8 -*-
"""pyspark_Tanhimislam_RDD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ljKXFzsOHXuYswX-YUUGRhE9Hze8N_-P

# **Installing pyspark**
"""

!pip install pyspark

sc.stop()
from pyspark import SparkContext, SparkConf

# #Spark Config
conf = SparkConf().setAppName("sample_app")
sc = SparkContext(conf=conf)

from google.colab import files
uploaded = files.upload()

"""**Applying Transformation and Action**"""

import io
rdd = sc.textFile("New.txt")

rdd.take(10)

"""# **General transformations** 

**Transformation: map and flatMap**
"""

def Func(Lines):
  Lines=Lines.lower()
  Lines=Lines.split()
  return Lines
rdd1=rdd.map(Func)

rdd1.take(5)

"""**Using flatMap Transformation**"""

rdd2=rdd.flatMap(Func)
rdd2.take(5)

"""**Transformation: filter**

I want to remove the words, which are not necessary to analyze this text. We call these words as **“stop words”**; Stop words do not add much value in a text. For example, “is”, “am”, “are” and “the” are few examples of stop words.
"""

stopwords = ['is','am','are','the','for','a','from','and','have','an','as']
rdd3 = rdd2.filter(lambda x: x not in stopwords)
rdd3.take(10)

"""**Transformation: groupBy**

After getting the results into rdd3, we want to group the words in rdd3 based on which letters they start with. For example, suppose I want to group each word of rdd3 based on first 3 characters.
"""

rdd4 = rdd3.groupBy(lambda w: w[0:3])

[(k, list(v)) for (k, v) in rdd4.take(10)]

"""In the below code I am first converting “rdd3” into “rdd3_mapped”.  The “rdd3_mapped” is nothing but a mapped (key,val) pair RDD. Then I am applying “groupByKey” transformation on “rdd3_mapped” to group the all elements based on the keys (words). Next, I am saving the result into “rdd3_grouped”. Let’s see the first 5 elements in “rdd3_grouped”."""

rdd3_mapped = rdd3.map(lambda x: (x,1))
rdd3_grouped = rdd3_mapped.groupByKey()

print(list((j[0], list(j[1])) for j in rdd3_grouped.take(20)))

"""After seeing the result of the above code, I rechecked the corpus to know, how many times the word ‘manager’ is there, so I found that ‘manager’ is written more then once. I figure out that there are more words like ‘manager.’ , ‘manager,’ and ”manager:’. Let’s filter ‘manager,’ in “rdd3”."""

rdd3.filter(lambda x: x == 'mountains,').collect()

"""We can see that in above output, we have multiple words with ‘manager’ in our corpus. To overcome this situation we can do several things. We could apply a regular expression to remove unnecessary punctuation from the words. For the purpose of this article, I am skipping that part.

Until now we have not calculated the frequencies / counts of each words. Let’s proceed further :
"""

rdd3_freq_of_words = rdd3_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)

"""In the above code, I first applied “mapValues” transformation on “rdd3_grouped”. The “mapValues” (only applicable on pair RDD) transformation is like a map (can be applied on any RDD) transform but it has one difference that when we apply map transform on pair RDD we can access the key and value both of this RDD but in case of “mapValues” transformation, it will transform the values by applying some function and key will not be affected. So for example, in above code I applied sum, which will calculate the sum (counts) for the each word.

After applying “mapValues”  transformation I want to sort the words based on their frequencies so for doing that I am first converting a ( word, frequency ) pair to ( frequency,word ) so that our key and values will be interchanged then, I will apply a sorting based on key and then get a result in “rdd3_freq_of_words”. We can see that 10 most frequent words I used in my previous blog by applying “take” action.
"""

rdd3_freq_of_words.take(10)

"""Transformation: mapPartitions

Q5: How do I perform a task (say count the words ‘far’ and ‘blind’ in rdd3) separatly on each partition and get the output of the task performed in these partition ?
Soltion: We can do this by applying “mapPartitions” transformation. The “mapPartitions” is like a map transformation but runs separately on different partitions of a RDD. So, for counting the frequencies of words ‘spark’ and ‘apache’ in each partition of RDD, you can follow the steps:
"""

def func(iterator):
  count_far = 0
  count_blind = 0
  for i in iterator:
     if i =='far':
        count_far = count_far + 1
     if i == 'blind':
        count_blind = count_blind + 1
  return (count_far,count_blind)

rdd3.mapPartitions(func).glom().collect()

"""I have used the “glom” function which is very useful when we want to see the data insights for each partition of a RDD. So above result shows that 4,4 are the counts of ‘far’, ‘blind’ in partition1 and 0,0 are the counts of ‘far’, ‘blind’ in partition2. If we won’t use the “glom” function we won’t we able to see the results of each partition."""

rdd3.mapPartitions(func).collect()

"""**Math / Statistical Transformation**
Transformation: sample

Q6: What if I want to work with samples instead of full data ?
Soltion: “sample” transformation helps us in taking samples instead of working on full data. The sample method will return a new RDD, containing a statistical sample of the original RDD.
We can pass the arguments insights as the sample operation:

    “withReplacement = True” or False (to choose the sample with or without replacement)
    “fraction = x” ( x= .4 means we want to choose 40% of data in “rdd” ) and “seed” for reproduce the results.
"""

rdd3_sampled = rdd3.sample(False, 0.4, 42)

len(rdd3.collect()),len(rdd3_sampled.collect())

"""Set Theory / Relational Transformation
Transformation: union

Q 7: What if I want to create a RDD which contains all the elements (a.k.a. union) of two RDDs ?
Solution: To do so, we can use “union” transformation on two RDDs. In Spark “union” transformation will return a new RDD by taking the union of two RDDs. Please note that duplicate items will not be removed in the new RDD. To illustrate this.

1.   I am first going to create a two sample RDD ( say sample1, sample2 ) from the “rdd3” by taking 20% sample for each.
2. Apply a union transformation on sample1, sample2.
"""

sample1 = rdd3.sample(False,0.2,42)
sample2 =rdd3.sample(False,0.2,42)
union_of_sample1_sample2 = sample1.union(sample2)

"""Transformation: join

Q 8: If we want to join the two pair RDDs based on their key.
Solution: The “join” transformation can help us join two pairs of RDDs based on their key. To show that:
1.       First create the two sample (key,value) pair RDDs (“sample1”, “sample2”) from the “rdd3_mapped” same as I did for “union” transformation
2.   Apply a “join” transformation on “sample1”,  “sample2”.
"""

sample1 = rdd3_mapped.sample(False,.2,42)
sample2 = rdd3_mapped.sample(False,.2,42)
join_on_sample1_sample2 = sample1.join(sample2)
join_on_sample1_sample2.take(2)

"""Transformation: distinct

Q 9: How to calculate distinct elements in a RDD ?
Solution: We can apply “distinct” transformation on RDD to get the distinct elements. Let’s see how many distinct words do we have in the “rdd3”.
"""

rdd3_distinct = rdd3.distinct()
len(rdd3_distinct.collect())

"""Data Structure / I/O Transformation
Transformation: coalesce

Q 10: What if I want to reduce the number of partition of a RDD and get the result in a new RDD?
Solution: We will use “coalesce” transformation here. To demonstrate that:

    Let’s first check the number of partition in rdd3.
"""

rdd3.getNumPartitions()

"""And now apply coalesce transformation on “rdd3” , get the results in “rdd3_coalesce” and see the number of partitions."""

rdd3_coalesce = rdd3.coalesce(1)
rdd3_coalesce.getNumPartitions()

""",”take” to print the first n elements of a RDD , “getNumPartitions” to know how many partition a RDD has and “collect” to print all elements of RDD.

# **General Actions**
Action: **getNumPartitions**

Q 11: How do I find out number of parition in RDD ?

Solution: With “getNumPartitions”, we can find out that how many partitions exist in our RDD. Let’s see how many partition our initial RDD ("rdd3") has.
"""

rdd3.getNumPartitions()

"""Action: **Reduce**

Q 12: If I want to find out the sum the all numbers in a RDD.

Solution: To demonstrate this, I will:

    First create a RDD from a list of number from (1,1000) called “num_rdd”.
    Use a reduce action and pass a function through it (lambda x,y:  x+y).

A reduce action is use for aggregating all the elements of RDD by applying pairwise user function.
"""

num_rdd = sc.parallelize(range(1,1000))
num_rdd.reduce(lambda x,y: x+y)

"""# Mathematical / Statistical Actions
** Action: count**

Q 13: Count the number of elements in RDD.

Solution: The count action will count the number of elements in RDD. To see that, let’s apply count action on “rdd3” to count the number of words in "rdd3".
"""

rdd3.count()

"""Action: max, min, sum, variance and stdev"""

num_rdd.max(),num_rdd.min(), num_rdd.sum(),num_rdd.variance(),num_rdd.stdev()